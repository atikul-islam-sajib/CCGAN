{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import zipfile\n",
    "import argparse\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomException(Exception):\n",
    "    def __init__(self, message: str):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        CustomException(\"Cannot be possble to dump the value\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename: str):\n",
    "    if isinstance(filename, str):\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        CustomException(\"Cannot be possble to load the value\".capitalize())\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def validate_path(path: str):\n",
    "    if isinstance(path, str):\n",
    "        if os.path.exists(path):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        CustomException(\"Cannot be possble to validate the path\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path=None,\n",
    "        image_size: int = 128,\n",
    "        channels: int = 3,\n",
    "        batch_size: int = 8,\n",
    "        split_size: float = 0.20,\n",
    "        seed_value: int = 0,\n",
    "    ):\n",
    "        self.image_path = image_path\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "        self.seed_value = seed_value\n",
    "\n",
    "        self.CONFIG = config()\n",
    "\n",
    "        self.independent: list = []\n",
    "        self.dependent: list = []\n",
    "        self.lr_independent: list = []\n",
    "        self.lr_dependent: list = []\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if validate_path(path=self.CONFIG[\"path\"][\"RAW_IMAGE_DATA_PATH\"]):\n",
    "            with zipfile.ZipFile(file=self.image_path, mode=\"r\") as zip_file:\n",
    "                zip_file.extractall(path=self.CONFIG[\"path\"][\"RAW_IMAGE_DATA_PATH\"])\n",
    "\n",
    "        else:\n",
    "            raise CustomException(\"Raw data path cannot be found\".capitalize())\n",
    "\n",
    "    def transforms(self, type=\"lr\"):\n",
    "        if type == \"hr\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif type == \"lr\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size // 4, self.image_size // 4)),\n",
    "                    transforms.CenterCrop((self.image_size // 4, self.image_size // 4)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def split_dataset(self, X: list, y: list):\n",
    "        if isinstance(X, list) and isinstance(y, list):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_size, random_state=self.seed_value\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "        else:\n",
    "            raise CustomException(\"X and y should be list\".capitalize())\n",
    "\n",
    "    def feature_extractor(self):\n",
    "        self.directory = os.path.join(\n",
    "            self.CONFIG[\"path\"][\"RAW_IMAGE_DATA_PATH\"], \"dataset\"\n",
    "        )\n",
    "        assert (\n",
    "            self.directory.split(\"/\")[-1] == \"dataset\"\n",
    "        ), \"Directory name should be dataset\"\n",
    "\n",
    "        self.X = os.path.join(self.directory, \"X\")\n",
    "        self.y = os.path.join(self.directory, \"y\")\n",
    "\n",
    "        print(self.X, self.y)\n",
    "\n",
    "        assert (\n",
    "            self.X.split(\"/\")[-1] == \"X\" and self.y.split(\"/\")[-1] == \"y\"\n",
    "        ), \"Directory name should be X and y\"\n",
    "\n",
    "        for _, image in tqdm(enumerate(os.listdir(self.X))):\n",
    "            if image in os.listdir(self.y):\n",
    "                self.imageX = os.path.join(self.X, image)\n",
    "                self.imageY = os.path.join(self.y, image)\n",
    "\n",
    "                self.imageX = cv2.imread(self.imageX)\n",
    "                self.imageY = cv2.imread(self.imageY)\n",
    "\n",
    "                self.imageX = cv2.cvtColor(self.imageX, cv2.COLOR_BGR2RGB)\n",
    "                self.imageY = cv2.cvtColor(self.imageY, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                self.imageX = Image.fromarray(self.imageX)\n",
    "                self.imageY = Image.fromarray(self.imageY)\n",
    "\n",
    "                self._imageX = self.transforms(type=\"hr\")(self.imageX)\n",
    "                self._imageY = self.transforms(type=\"hr\")(self.imageY)\n",
    "\n",
    "                self.lr_imageX = self.transforms(type=\"lr\")(self.imageX)\n",
    "                self.lr_imageY = self.transforms(type=\"lr\")(self.imageY)\n",
    "\n",
    "                self.independent.append(self._imageX)\n",
    "                self.dependent.append(self._imageY)\n",
    "\n",
    "                self.lr_independent.append(self.lr_imageX)\n",
    "                self.lr_dependent.append(self.lr_imageY)\n",
    "\n",
    "        assert (\n",
    "            len(self.independent)\n",
    "            == len(self.dependent)\n",
    "            == len(self.lr_dependent)\n",
    "            == len(self.lr_dependent)\n",
    "        ), \"Length of independent and dependent should be equal\"\n",
    "\n",
    "        try:\n",
    "            dataset = self.split_dataset(X=self.independent, y=self.dependent)\n",
    "            lr_dataset = self.split_dataset(X=self.lr_independent, y=self.lr_dependent)\n",
    "        except CustomException as e:\n",
    "            print(\"An error occurred: \", e)\n",
    "            traceback.print_exc()\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred: \", e)\n",
    "            traceback.print_exc()\n",
    "        else:\n",
    "            return dataset, lr_dataset\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        dataset, lr_dataset = self.feature_extractor()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=list(\n",
    "                zip(dataset[\"X_train\"], dataset[\"y_train\"], lr_dataset[\"y_train\"])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            dataset=list(\n",
    "                zip(dataset[\"X_test\"], dataset[\"y_test\"], lr_dataset[\"y_test\"])\n",
    "            ),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        if validate_path(path=self.CONFIG[\"path\"][\"PROCESSED_IMAGE_DATA_PATH\"]):\n",
    "            for value, filename in [\n",
    "                (train_dataloader, \"train_dataloader.pkl\"),\n",
    "                (valid_dataloader, \"valid_dataloader.pkl\"),\n",
    "            ]:\n",
    "                dump(\n",
    "                    value=value,\n",
    "                    filename=os.path.join(\n",
    "                        self.CONFIG[\"path\"][\"PROCESSED_IMAGE_DATA_PATH\"], filename\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"Train and valid dataloader is saved in the folder {}\".format(\n",
    "                    self.CONFIG[\"path\"][\"PROCESSED_IMAGE_DATA_PATH\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise CustomException(\n",
    "                \"Cannot be created the dataloader and processed path is not found\".capitalize()\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_images():\n",
    "        processed_data_path = config()[\"path\"][\"PROCESSED_IMAGE_DATA_PATH\"]\n",
    "        if validate_path(path=processed_data_path):\n",
    "            train_dataloader = load(\n",
    "                filename=os.path.join(processed_data_path, \"train_dataloader.pkl\")\n",
    "            )\n",
    "\n",
    "            X, y, lr = next(iter(train_dataloader))\n",
    "\n",
    "            num_of_rows = X.size(0) // 2\n",
    "            num_of_columns = X.size(0) // num_of_rows\n",
    "\n",
    "            plt.figure(figsize=(20, 30))\n",
    "\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            for index, image in enumerate(X):\n",
    "                imageX = image.permute(1, 2, 0).detach().numpy()\n",
    "                imageY = y[index].permute(1, 2, 0).detach().numpy()\n",
    "                lowerX = lr[index].permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "                imageX = (imageX - imageX.min()) / (imageX.max() - imageX.min())\n",
    "                imageY = (imageY - imageY.min()) / (imageY.max() - imageY.min())\n",
    "                lowerX = (lowerX - lowerX.min()) / (lowerX.max() - lowerX.min())\n",
    "\n",
    "                plt.subplot(3 * num_of_rows, 3 * num_of_columns, 3 * index + 1)\n",
    "                plt.imshow(imageX)\n",
    "                plt.title(\"X\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.subplot(3 * num_of_rows, 3 * num_of_columns, 3 * index + 2)\n",
    "                plt.imshow(imageY)\n",
    "                plt.title(\"Y\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.subplot(3 * num_of_rows, 3 * num_of_columns, 3 * index + 3)\n",
    "                plt.imshow(lowerX)\n",
    "                plt.title(\"lowerY\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(config()[\"path\"][\"FILES_PATH\"], \"image.jpeg\"))\n",
    "            plt.show()\n",
    "\n",
    "            print(\n",
    "                \"Image is saved in the folder {}\".format(config()[\"path\"][\"FILES_PATH\"])\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise CustomException(\n",
    "                \"Cannot be imported processed path as it is not found\".capitalize()\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_details():\n",
    "        processed_data_path = config()[\"path\"][\"PROCESSED_IMAGE_DATA_PATH\"]\n",
    "        if validate_path(path=processed_data_path):\n",
    "            train_dataloader = load(\n",
    "                filename=os.path.join(processed_data_path, \"train_dataloader.pkl\")\n",
    "            )\n",
    "            valid_dataloader = load(\n",
    "                filename=os.path.join(processed_data_path, \"valid_dataloader.pkl\")\n",
    "            )\n",
    "\n",
    "            trainX, trainY, train_lr_Y = next(iter(train_dataloader))\n",
    "            validX, validY, valid_lr_Y = next(iter(valid_dataloader))\n",
    "\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"total_dataset\": str(\n",
    "                        sum(X.size(0) for X, _, _ in train_dataloader)\n",
    "                        + sum(X.size(0) for X, _, _ in valid_dataloader)\n",
    "                    ),\n",
    "                    \"trainX(shape)\": str(trainX.size()),\n",
    "                    \"trainY(shape)\": str(trainY.size()),\n",
    "                    \"validX(shape)\": str(validX.size()),\n",
    "                    \"validY(shape)\": str(validY.size()),\n",
    "                    \"train_lr_Y(shape)\": str(train_lr_Y.size()),\n",
    "                    \"valid_lr_Y(shape)\": str(valid_lr_Y.size()),\n",
    "                },\n",
    "                index=[\"Deatils\".title()],\n",
    "            ).T.to_csv(\n",
    "                os.path.join(config()[\"path\"][\"FILES_PATH\"], \"dataset_details.csv\")\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"Dataset details are saved in the folder {}\".format(\n",
    "                    config()[\"path\"][\"FILES_PATH\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Dataloader for the CCGAN\".title())\n",
    "    parser.add_argument(\n",
    "        \"--image_path\",\n",
    "        type=str,\n",
    "        default=config()[\"dataloader\"][\"image_path\"],\n",
    "        help=\"Batch size for the dataloader\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--channels\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"channels\"],\n",
    "        help=\"Number of channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_size\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"image_size\"],\n",
    "        help=\"Image size\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"batch_size\"],\n",
    "        help=\"Batch size\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_size\",\n",
    "        type=float,\n",
    "        default=config()[\"dataloader\"][\"split_size\"],\n",
    "        help=\"Split ratio\".capitalize(),\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    loader = Loader(\n",
    "        image_path=args.image_path,\n",
    "        channels=args.channels,\n",
    "        image_size=args.image_size,\n",
    "        batch_size=args.batch_size,\n",
    "        split_size=args.split_size,\n",
    "    )\n",
    "    # try:\n",
    "    #     loader.unzip_folder()\n",
    "    # except CustomException as e:\n",
    "    #     print(e)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "\n",
    "    try:\n",
    "        loader.create_dataloader()\n",
    "    except CustomException as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 64,\n",
    "        batchnorm: bool = True,\n",
    "        leakyrelu: bool = True,\n",
    "    ):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.batchnorm = batchnorm\n",
    "        self.leakyrelu = leakyrelu\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "        self.momentum = 0.8\n",
    "        self.negative_slope = 0.2\n",
    "\n",
    "        self.encoder_block = self.block()\n",
    "\n",
    "    def block(self):\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            )\n",
    "        )\n",
    "        if self.batchnorm:\n",
    "            self.layers.append(\n",
    "                nn.BatchNorm2d(num_features=self.out_channels, momentum=self.momentum)\n",
    "            )\n",
    "        if self.leakyrelu:\n",
    "            self.layers.append(\n",
    "                nn.LeakyReLU(negative_slope=self.negative_slope, inplace=True)\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.encoder_block(x)\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Encoder Block for Generator\".title())\n",
    "    parser.add_argument(\n",
    "        \"--in_channels\", type=int, default=3, help=\"Input Channels\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_channels\", type=int, default=64, help=\"Output Channels\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batchnorm\", type=bool, default=True, help=\"Batch Normalization\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--leakyrelu\", type=bool, default=False, help=\"Leaky ReLU\".title()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    encoder = EncoderBlock(\n",
    "        in_channels=args.in_channels,\n",
    "        out_channels=args.out_channels,\n",
    "        batchnorm=args.batchnorm,\n",
    "        leakyrelu=args.leakyrelu,\n",
    "    )\n",
    "\n",
    "    assert encoder(torch.randn(1, 3, 128, 128)).size() == torch.Size(\n",
    "        [1, 64, 64, 64]\n",
    "    ), \"Encoder Block is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 512,\n",
    "        out_channels: int = 512,\n",
    "        batchnorm: bool = True,\n",
    "        leakyrelu: bool = True,\n",
    "    ):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.batchnorm = batchnorm\n",
    "        self.leakyrelu = leakyrelu\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "        self.momentum = 0.8\n",
    "\n",
    "        self.decoder_block = self.block()\n",
    "\n",
    "    def block(self):\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.batchnorm:\n",
    "            self.layers.append(\n",
    "                nn.BatchNorm2d(num_features=self.out_channels, momentum=self.momentum)\n",
    "            )\n",
    "\n",
    "        if self.leakyrelu:\n",
    "            self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        else:\n",
    "            self.layers.append(nn.Tanh())\n",
    "\n",
    "        return nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.decoder_block(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Decoder Block for Generator\".title())\n",
    "    parser.add_argument(\n",
    "        \"--in_channels\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Number of input channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_channels\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Number of output channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batchnorm\", type=bool, default=True, help=\"Batch Normalization\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--leakyrelu\", type=bool, default=True, help=\"Leaky ReLU\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    decoder = DecoderBlock(\n",
    "        in_channels=args.in_channels,\n",
    "        out_channels=args.out_channels,\n",
    "        batchnorm=args.batchnorm,\n",
    "        leakyrelu=args.leakyrelu,\n",
    "    )\n",
    "\n",
    "    assert decoder(torch.randn(1, 512, 2, 2)).size() == torch.Size(\n",
    "        [1, 512, 4, 4]\n",
    "    ), \"Decoder Block is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_size: tuple = (1, 3, 128, 128)):\n",
    "        super(Generator, self).__init__()\n",
    "        self.batch_size, self.channels, self.image_height, self.image_width = image_size\n",
    "\n",
    "        self.encoder1 = EncoderBlock(\n",
    "            in_channels=self.channels,\n",
    "            out_channels=self.image_height // 2,\n",
    "            batchnorm=False,\n",
    "        )\n",
    "        self.encoder2 = EncoderBlock(\n",
    "            in_channels=self.image_height // 2,\n",
    "            out_channels=self.image_height,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.encoder3 = EncoderBlock(\n",
    "            in_channels=self.image_height + self.channels,\n",
    "            out_channels=self.image_height * 2,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.encoder4 = EncoderBlock(\n",
    "            in_channels=self.image_height * 2,\n",
    "            out_channels=self.image_height * 4,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.encoder5 = EncoderBlock(\n",
    "            in_channels=self.image_height * 4,\n",
    "            out_channels=self.image_height * 4,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.encoder6 = EncoderBlock(\n",
    "            in_channels=self.image_height * 4,\n",
    "            out_channels=self.image_height * 4,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(\n",
    "            in_channels=self.image_height * 4, out_channels=self.image_height * 4\n",
    "        )\n",
    "        self.decoder2 = DecoderBlock(\n",
    "            in_channels=self.image_height * 8, out_channels=self.image_height * 4\n",
    "        )\n",
    "        self.decoder3 = DecoderBlock(\n",
    "            in_channels=self.image_height * 8, out_channels=self.image_height * 2\n",
    "        )\n",
    "        self.decoder4 = DecoderBlock(\n",
    "            in_channels=self.image_height * 4, out_channels=self.image_height\n",
    "        )\n",
    "        self.decoder5 = DecoderBlock(\n",
    "            in_channels=self.image_height * 2 + self.channels,\n",
    "            out_channels=self.image_height // 2,\n",
    "        )\n",
    "        self.decoder6 = DecoderBlock(\n",
    "            in_channels=self.image_height, out_channels=self.channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lr_image: torch.Tensor):\n",
    "        if (isinstance(x, torch.Tensor)) and (isinstance(lr_image, torch.Tensor)):\n",
    "            encoder1 = self.encoder1(x)\n",
    "\n",
    "            encoder2 = self.encoder2(encoder1)\n",
    "\n",
    "            _encoder2 = torch.cat((encoder2, lr_image), dim=1)\n",
    "\n",
    "            encoder3 = self.encoder3(_encoder2)\n",
    "            encoder3 = torch.dropout(input=encoder3, p=0.5, train=self.training)\n",
    "\n",
    "            encoder4 = self.encoder4(encoder3)\n",
    "            encoder4 = torch.dropout(input=encoder4, p=0.5, train=self.training)\n",
    "\n",
    "            encoder5 = self.encoder5(encoder4)\n",
    "            encoder5 = torch.dropout(input=encoder5, p=0.5, train=self.training)\n",
    "\n",
    "            encoder6 = self.encoder6(encoder5)\n",
    "            encoder6 = torch.dropout(input=encoder6, p=0.5, train=self.training)\n",
    "\n",
    "            decoder1 = torch.cat((self.decoder1(encoder6), encoder5), dim=1)\n",
    "            decoder1 = torch.dropout(input=decoder1, p=0.5, train=self.training)\n",
    "\n",
    "            decoder2 = torch.cat((self.decoder2(decoder1), encoder4), dim=1)\n",
    "            decoder2 = torch.dropout(input=decoder2, p=0.5, train=self.training)\n",
    "\n",
    "            decoder3 = torch.cat((self.decoder3(decoder2), encoder3), dim=1)\n",
    "            decoder3 = torch.dropout(input=decoder3, p=0.5, train=self.training)\n",
    "\n",
    "            decoder4 = torch.cat((self.decoder4(decoder3), _encoder2), dim=1)\n",
    "\n",
    "            decoder5 = torch.cat((self.decoder5(decoder4), encoder1), dim=1)\n",
    "\n",
    "            output = self.decoder6(decoder5)\n",
    "\n",
    "            assert output.size() == (\n",
    "                self.batch_size,\n",
    "                self.channels,\n",
    "                self.image_height,\n",
    "                self.image_width,\n",
    "            ), \"Image size is incorrect in Generator\".capitalize()\n",
    "\n",
    "            return output\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model=None):\n",
    "        if isinstance(model, Generator):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Model must be of type Generator\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = config()[\"dataloader\"][\"batch_size\"]\n",
    "    channels = config()[\"dataloader\"][\"channels\"]\n",
    "    image_size = config()[\"dataloader\"][\"image_size\"]\n",
    "\n",
    "    image = (batch_size, channels, image_size, image_size)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Generator for CCGAN\".title())\n",
    "    parser.add_argument(\n",
    "        \"--image_size\",\n",
    "        type=parse_tuple,\n",
    "        default=image,\n",
    "        help=\"Image size (e.g., '(1,3,128,128)')\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    netG = Generator(image_size=args.image_size)\n",
    "\n",
    "    assert (\n",
    "        netG(\n",
    "            torch.randn(args.image_size),\n",
    "            torch.randn(\n",
    "                args.image_size[0],\n",
    "                args.image_size[1],\n",
    "                args.image_size[2] // 4,\n",
    "                args.image_size[3] // 4,\n",
    "            ),\n",
    "        ).size()\n",
    "    ) == (args.image_size), \"Image size is incorrect in Generator\".capitalize()\n",
    "\n",
    "    print(\"Total params of the netG = {}\".format(Generator.total_params(netG)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
